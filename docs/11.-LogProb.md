## Log Probability via Completions

TabbyAPI exposes token log probability scoring through the `/v1/completions`
route.  When `max_tokens` is `0`, only the prompt is scored.  With
`max_tokens` greater than `0`, the completion tokens are scored as well.  The
older `/v1/logprob` route is still available for compatibility.

### Request
`POST /v1/completions`

```json
{
  "prompt": "Hello world",
  "max_tokens": 0,
  "logprobs": 2,
  "echo": true
}
```

`logprobs` accepts values from `1` to `5`. `echo` controls whether the prompt
text is returned. Set it to `true` to mimic OpenAI's behaviour. The former `n`
parameter from `/v1/logprob` is no longer required. Parameters such as
`best_of`, `suffix`, and `user` are parsed only for OpenAI compatibility and are
otherwise ignored.

When `echo` is enabled, prompt tokens are listed first in all `logprobs` arrays
(`tokens`, `token_logprobs`, `top_logprobs`, `text_offset`), followed by any 
completion tokens. This ensures consistent ordering across all logprob data 
and matches OpenAI API behavior.

### Response

```json
{
  "id": "cmpl-...",
  "object": "text_completion",
  "created": 1710000000,
  "model": "model-name",
  "choices": [
    {
      "index": 0,
      "text": "Hello world",
      "logprobs": {
        "tokens": ["Hello", "world"],
        "token_logprobs": [-1.5, -2.0]
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 2,
    "completion_tokens": 0,
    "total_tokens": 2
  }
}
```